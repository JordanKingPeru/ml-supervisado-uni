# üå≤ Sesi√≥n 02: La Revoluci√≥n No-Lineal (Trees, SVM & Ensembles)

**Fecha:** Domingo 22 de Junio | **Horario:** 09:00 - 13:00

## üéØ Objetivos de la Sesi√≥n
Al finalizar esta sesi√≥n, dejar√°s de ver los algoritmos como "cajas negras" y podr√°s:
1.  **Entender la No-Linealidad:** Por qu√© una Regresi√≥n Log√≠stica falla donde un Random Forest brilla.
2.  **Dominar los √Årboles:** Controlar su crecimiento (Pruning) para evitar el overfitting.
3.  **Transici√≥n a Boosting:** Entender por qu√© XGBoost/LightGBM son el est√°ndar en competencias (Kaggle).
4.  **Ubicar a los Cl√°sicos (SVM/KNN):** Saber cu√°ndo usarlos (ej. imputaci√≥n, datasets peque√±os) y cu√°ndo no.

## üìÇ Estructura del Material
*   **`slides/`**: Presentaci√≥n te√≥rica de la sesi√≥n.
*   **`notebooks/`**:
    *   `01_Algoritmos_No_Lineales.ipynb`: Notebook maestro con demos visuales (Moons, Trees, SVM) y teor√≠a aplicada.
    *   `02_Arena_Combate.ipynb`: Taller competitivo (Random Forest vs LightGBM).
*   **`data/`**: Dataset `telco_churn.csv`.

## üõ†Ô∏è Conceptos Clave
*   **Decision Tree:** `DecisionTreeClassifier`, `plot_tree`
*   **Ensemble Learning:** Bagging (Random Forest) vs Boosting (Gradient Boosting)
*   **SOTA Algorithms:** `XGBoost`, `LightGBM`, `CatBoost`
*   **Algoritmos Geom√©tricos:** `SVC` (SVM), `KNeighborsClassifier` (KNN)
*   **Visualizaci√≥n:** `mlxtend.plotting.plot_decision_regions`

## üìö Tarea para la casa
Experimentar con los hiperpar√°metros de LightGBM (`num_leaves`, `learning_rate`) en el notebook de Arena de Combate para intentar superar el baseline.
