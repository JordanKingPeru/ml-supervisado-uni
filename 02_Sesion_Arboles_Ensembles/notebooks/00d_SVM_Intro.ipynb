{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e7acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Imports y Configuraci√≥n\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Configuraci√≥n Visual\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b024256",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. La Intuici√≥n: M√°ximo Margen\n",
    "\n",
    "### ¬øPor Qu√© Maximizar el Margen?\n",
    "\n",
    "Infinitos hiperplanos pueden separar dos clases. SVM elige el que tiene el **margen m√°s grande**.\n",
    "\n",
    "```\n",
    "                    ‚úì SVM (margen m√°ximo)\n",
    "    ‚óè  ‚óè           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    ‚óè     ‚óè        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê  ‚Üê Margen\n",
    "                   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "         ‚óã  ‚óã  ‚óã\n",
    "           ‚óã\n",
    "```\n",
    "\n",
    "### Matem√°ticamente\n",
    "\n",
    "- **Hiperplano:** $\\mathbf{w} \\cdot \\mathbf{x} + b = 0$\n",
    "- **Margen:** $\\frac{2}{||\\mathbf{w}||}$\n",
    "- **Objetivo:** Minimizar $||\\mathbf{w}||^2$ sujeto a clasificar correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6904fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n: M√∫ltiples separadores vs SVM\n",
    "np.random.seed(RANDOM_STATE)\n",
    "X_simple = np.vstack([np.random.randn(20, 2) + [2, 2],\n",
    "                      np.random.randn(20, 2) + [-2, -2]])\n",
    "y_simple = np.array([0]*20 + [1]*20)\n",
    "\n",
    "# Entrenar SVM lineal\n",
    "svm_linear = SVC(kernel='linear', C=1000)  # C alto = hard margin\n",
    "svm_linear.fit(X_simple, y_simple)\n",
    "\n",
    "# Funci√≥n para visualizar SVM\n",
    "\n",
    "\n",
    "def plot_svm_decision(model, X, y, ax, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Frontera de decisi√≥n y m√°rgenes\n",
    "    ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1],\n",
    "               alpha=0.8, linestyles=['--', '-', '--'])\n",
    "    ax.contourf(xx, yy, Z, levels=[-100, 0, 100],\n",
    "                alpha=0.2, colors=['blue', 'red'])\n",
    "\n",
    "    # Puntos\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolors='k', s=100)\n",
    "\n",
    "    # Support Vectors\n",
    "    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "               s=300, linewidth=2, facecolors='none', edgecolors='yellow')\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_svm_decision(svm_linear, X_simple, y_simple, ax,\n",
    "                  'SVM Lineal: M√°ximo Margen\\n(c√≠rculos amarillos = Support Vectors)')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä N√∫mero de Support Vectors: {len(svm_linear.support_vectors_)}\")\n",
    "print(\"üí° Solo estos puntos definen la frontera. El resto es irrelevante.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a116696",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Support Vectors: Los Puntos Cr√≠ticos\n",
    "\n",
    "### ¬øQu√© son los Support Vectors?\n",
    "\n",
    "Son los puntos que **\"tocan\" el margen**. Si los eliminas, la frontera cambia.\n",
    "\n",
    "| Propiedad | Implicaci√≥n |\n",
    "|-----------|-------------|\n",
    "| **Pocos SV** | Modelo simple, buena generalizaci√≥n |\n",
    "| **Muchos SV** | Modelo complejo, posible overfitting |\n",
    "| **Solo SV importan** | El resto de puntos pueden eliminarse sin cambiar el modelo |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdffb95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n: Solo los Support Vectors definen el modelo\n",
    "# Eliminar puntos que NO son support vectors\n",
    "sv_indices = svm_linear.support_\n",
    "X_only_sv = X_simple[sv_indices]\n",
    "y_only_sv = y_simple[sv_indices]\n",
    "\n",
    "svm_from_sv = SVC(kernel='linear', C=1000)\n",
    "svm_from_sv.fit(X_only_sv, y_only_sv)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "plot_svm_decision(svm_linear, X_simple, y_simple, axes[0],\n",
    "                  f'Todos los datos ({len(X_simple)} puntos)')\n",
    "\n",
    "plot_svm_decision(svm_from_sv, X_only_sv, y_only_sv, axes[1],\n",
    "                  f'Solo Support Vectors ({len(X_only_sv)} puntos)')\n",
    "\n",
    "plt.suptitle('¬°El modelo es el mismo!', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d1ce13",
   "metadata": {},
   "source": [
    "> **üí° Pro-Tip: Support Vectors y Memoria**\n",
    "> SVM solo necesita guardar los support vectors para hacer predicciones. Si tienes 1M de datos pero solo 1000 SV, el modelo es compacto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf65b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. El Kernel Trick: Magia Dimensional\n",
    "\n",
    "### El Problema: Datos No Linealmente Separables\n",
    "\n",
    "¬øQu√© pasa cuando los datos tienen forma de c√≠rculos conc√©ntricos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb9175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos en forma de c√≠rculos\n",
    "X_circles, y_circles = make_circles(\n",
    "    n_samples=200, noise=0.1, factor=0.3, random_state=RANDOM_STATE)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Kernel Lineal (falla)\n",
    "svm_lin = SVC(kernel='linear')\n",
    "svm_lin.fit(X_circles, y_circles)\n",
    "plot_svm_decision(svm_lin, X_circles, y_circles, axes[0],\n",
    "                  f'Kernel Lineal\\nAcc: {svm_lin.score(X_circles, y_circles):.2%}')\n",
    "\n",
    "# Kernel Polinomial\n",
    "svm_poly = SVC(kernel='poly', degree=3)\n",
    "svm_poly.fit(X_circles, y_circles)\n",
    "plot_svm_decision(svm_poly, X_circles, y_circles, axes[1],\n",
    "                  f'Kernel Polinomial (grado 3)\\nAcc: {svm_poly.score(X_circles, y_circles):.2%}')\n",
    "\n",
    "# Kernel RBF (Gaussiano)\n",
    "svm_rbf = SVC(kernel='rbf', gamma='auto')\n",
    "svm_rbf.fit(X_circles, y_circles)\n",
    "plot_svm_decision(svm_rbf, X_circles, y_circles, axes[2],\n",
    "                  f'Kernel RBF (Gaussiano)\\nAcc: {svm_rbf.score(X_circles, y_circles):.2%}')\n",
    "\n",
    "plt.suptitle('El Poder del Kernel: Transformar sin Calcular', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae06b291",
   "metadata": {},
   "source": [
    "### ¬øC√≥mo Funciona el Kernel Trick?\n",
    "\n",
    "**Idea:** Mapear los datos a un espacio de **alta dimensi√≥n** donde sean linealmente separables.\n",
    "\n",
    "**El Truco:** No necesitamos calcular las coordenadas en ese espacio, solo el **producto interno** (kernel).\n",
    "\n",
    "| Kernel | F√≥rmula | Uso T√≠pico |\n",
    "|--------|---------|------------|\n",
    "| **Lineal** | $K(x,y) = x \\cdot y$ | Datos linealmente separables |\n",
    "| **Polinomial** | $K(x,y) = (x \\cdot y + c)^d$ | Interacciones de features |\n",
    "| **RBF (Gaussiano)** | $K(x,y) = e^{-\\gamma ||x-y||^2}$ | Default para no lineal |\n",
    "\n",
    "```\n",
    "Espacio Original (2D)        Espacio Transformado (‚àûD)\n",
    "    ‚óã ‚óã ‚óã                         ‚óã ‚óã ‚óã\n",
    "  ‚óè ‚óã ‚óã ‚óã ‚óè        ‚Üí        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    ‚óã ‚óã ‚óã                         ‚óè ‚óè ‚óè\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf4836",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. SVM en Acci√≥n\n",
    "\n",
    "### Comparaci√≥n con Dataset \"Moons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos moons\n",
    "X_moons, y_moons = make_moons(\n",
    "    n_samples=300, noise=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Escalar datos (IMPORTANTE para SVM)\n",
    "scaler = StandardScaler()\n",
    "X_moons_scaled = scaler.fit_transform(X_moons)\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_moons_scaled, y_moons, test_size=0.3, random_state=RANDOM_STATE)\n",
    "\n",
    "# Entrenar SVM RBF\n",
    "svm_moons = SVC(kernel='rbf', C=1, gamma='scale')\n",
    "svm_moons.fit(X_train, y_train)\n",
    "\n",
    "print(f\"üìä Train Accuracy: {svm_moons.score(X_train, y_train):.2%}\")\n",
    "print(f\"üìä Test Accuracy:  {svm_moons.score(X_test, y_test):.2%}\")\n",
    "print(\n",
    "    f\"üìä Support Vectors: {len(svm_moons.support_vectors_)} de {len(X_train)} ({len(svm_moons.support_vectors_)/len(X_train):.1%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b39387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar frontera\n",
    "def plot_svm_full(model, X, y, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolors='k', alpha=0.8)\n",
    "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "                s=200, linewidth=2, facecolors='none', edgecolors='yellow')\n",
    "    plt.title(title)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_svm_full(svm_moons, X_moons_scaled, y_moons,\n",
    "              f'SVM RBF en Moons\\nAcc: {svm_moons.score(X_test, y_test):.2%}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc926b",
   "metadata": {},
   "source": [
    "> **‚ö†Ô∏è Real-World Warning: ¬°Escala tus datos!**\n",
    "> SVM es MUY sensible a la escala. Si una feature va de 0-1 y otra de 0-1000000, el kernel RBF fallar√°.\n",
    "> \n",
    "> **Siempre:** `StandardScaler()` antes de SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a575dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Hiperpar√°metros Clave\n",
    "\n",
    "### Los Dos Par√°metros Cr√≠ticos\n",
    "\n",
    "| Par√°metro | Descripci√≥n | Efecto |\n",
    "|-----------|-------------|--------|\n",
    "| **C** | Regularizaci√≥n (inverso) | C bajo = margen ancho (tolerante), C alto = margen estrecho (estricto) |\n",
    "| **gamma** (Œ≥) | Alcance del kernel RBF | Œ≥ bajo = influencia amplia, Œ≥ alto = influencia local |\n",
    "\n",
    "```\n",
    "C bajo + Œ≥ bajo = Underfitting (muy simple)\n",
    "C alto + Œ≥ alto = Overfitting (muy complejo)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c0673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar efecto de C y gamma\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Variar C\n",
    "for i, C in enumerate([0.1, 1, 100]):\n",
    "    svm_c = SVC(kernel='rbf', C=C, gamma='scale')\n",
    "    svm_c.fit(X_moons_scaled, y_moons)\n",
    "    plt.sca(axes[0, i])\n",
    "    plot_svm_full(svm_c, X_moons_scaled, y_moons,\n",
    "                  f'C = {C}\\nSV: {len(svm_c.support_vectors_)}')\n",
    "\n",
    "# Variar gamma\n",
    "for i, gamma in enumerate([0.1, 1, 10]):\n",
    "    svm_g = SVC(kernel='rbf', C=1, gamma=gamma)\n",
    "    svm_g.fit(X_moons_scaled, y_moons)\n",
    "    plt.sca(axes[1, i])\n",
    "    plot_svm_full(svm_g, X_moons_scaled, y_moons,\n",
    "                  f'Œ≥ = {gamma}\\nSV: {len(svm_g.support_vectors_)}')\n",
    "\n",
    "axes[0, 0].set_ylabel('Variando C', fontsize=14)\n",
    "axes[1, 0].set_ylabel('Variando Œ≥', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç C alto ‚Üí Frontera m√°s compleja (menos tolerante a errores)\")\n",
    "print(\"üîç Œ≥ alto ‚Üí Puntos solo influyen localmente (frontera dentada)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05dce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search para encontrar mejores par√°metros\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(kernel='rbf'), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"üìä Mejores par√°metros: {grid_search.best_params_}\")\n",
    "print(f\"üìä Mejor CV Score: {grid_search.best_score_:.2%}\")\n",
    "print(f\"üìä Test Score: {grid_search.score(X_test, y_test):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63085632",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Caso de Negocio: Clasificaci√≥n de D√≠gitos\n",
    "\n",
    "SVM hist√≥ricamente fue muy popular para reconocimiento de d√≠gitos escritos a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac16037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Cargar datos\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "# Visualizar algunos ejemplos\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='gray')\n",
    "    ax.set_title(f'Etiqueta: {digits.target[i]}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Dataset de D√≠gitos (8x8 p√≠xeles)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Shape: {X_digits.shape} (cada imagen es 64 features = 8x8 p√≠xeles)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar y dividir\n",
    "scaler = StandardScaler()\n",
    "X_digits_scaled = scaler.fit_transform(X_digits)\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(\n",
    "    X_digits_scaled, y_digits, test_size=0.3, stratify=y_digits, random_state=RANDOM_STATE)\n",
    "\n",
    "# Entrenar SVM\n",
    "svm_digits = SVC(kernel='rbf', C=10, gamma='scale')\n",
    "svm_digits.fit(X_tr, y_tr)\n",
    "\n",
    "print(f\"üìä Test Accuracy: {svm_digits.score(X_ts, y_ts):.2%}\")\n",
    "print(\n",
    "    f\"üìä Support Vectors: {len(svm_digits.support_vectors_)} de {len(X_tr)} ({len(svm_digits.support_vectors_)/len(X_tr):.1%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7820c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver algunos errores\n",
    "y_pred = svm_digits.predict(X_ts)\n",
    "errors = y_pred != y_ts\n",
    "error_indices = np.where(errors)[0]\n",
    "\n",
    "if len(error_indices) > 0:\n",
    "    fig, axes = plt.subplots(1, min(5, len(error_indices)), figsize=(12, 3))\n",
    "    if len(error_indices) < 5:\n",
    "        axes = [axes] if len(error_indices) == 1 else axes\n",
    "    for i, idx in enumerate(error_indices[:5]):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(scaler.inverse_transform(\n",
    "            X_ts[idx:idx+1]).reshape(8, 8), cmap='gray')\n",
    "        ax.set_title(\n",
    "            f'Real: {y_ts.iloc[idx] if hasattr(y_ts, \"iloc\") else y_ts[idx]}\\nPred: {y_pred[idx]}')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(\n",
    "        'Errores del Modelo (¬øPuedes ver por qu√© fall√≥?)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"üéâ ¬°No hay errores en la muestra!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594bfee",
   "metadata": {},
   "source": [
    "### üß† Micro-Desaf√≠o: ¬øPor Qu√© SVM Brilla Aqu√≠?\n",
    "\n",
    "**Reflexiona:**\n",
    "1. Los datos est√°n en alta dimensi√≥n (64 features)\n",
    "2. Las clases son relativamente separables\n",
    "3. El kernel RBF captura patrones no lineales\n",
    "\n",
    "**Pregunta:** ¬øQu√© pasar√≠a si las im√°genes fueran de 100x100 p√≠xeles?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27246fe",
   "metadata": {},
   "source": [
    "> **‚ö†Ô∏è Real-World Warning: Escalabilidad**\n",
    "> SVM tiene complejidad $O(n^2)$ a $O(n^3)$ en memoria y tiempo. Con m√°s de ~50,000 muestras, considera:\n",
    "> - `LinearSVC` para kernel lineal\n",
    "> - Reducci√≥n de dimensionalidad (PCA)\n",
    "> - Aproximaciones como `SGDClassifier` con kernel aproximado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e509e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Resumen y Siguiente Paso\n",
    "\n",
    "### üèÜ Resumen de Logros\n",
    "¬°Felicidades! En este notebook has aprendido:\n",
    "\n",
    "1. **M√°ximo Margen:** SVM busca el separador con mayor distancia a los puntos m√°s cercanos.\n",
    "2. **Support Vectors:** Solo los puntos cr√≠ticos definen el modelo.\n",
    "3. **Kernel Trick:** Transformar el espacio sin calcular dimensiones infinitas.\n",
    "4. **C y gamma:** Controlan regularizaci√≥n y alcance del kernel.\n",
    "5. **Escalado:** Imprescindible para SVM.\n",
    "\n",
    "### ‚úÖ Cu√°ndo Usar SVM\n",
    "- Datos de dimensi√≥n media-alta (10-1000 features)\n",
    "- Clases bien separables\n",
    "- Cuando necesitas un modelo compacto (pocos SV)\n",
    "\n",
    "### ‚ö†Ô∏è Cu√°ndo Evitar SVM\n",
    "- Datasets muy grandes (>50k muestras)\n",
    "- Cuando necesitas probabilidades calibradas\n",
    "- Features categ√≥ricas sin encoding\n",
    "\n",
    "---\n",
    "\n",
    "### üëâ Siguiente Paso\n",
    "SVM busca el mejor separador global. Pero existe un enfoque m√°s simple: **¬øQu√© hacen tus vecinos m√°s cercanos?**\n",
    "\n",
    "**KNN (K-Nearest Neighbors):** Clasificar bas√°ndose en la votaci√≥n de los K vecinos m√°s cercanos.\n",
    "\n",
    "*En el siguiente notebook veremos este algoritmo \"lazy\" que no entrena, solo recuerda.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
