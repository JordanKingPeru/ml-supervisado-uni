{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44913284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Imports y Configuraci√≥n\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Configuraci√≥n Visual\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29252042",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. El Problema de la Varianza\n",
    "\n",
    "### Un √°rbol solo es \"inestable\"\n",
    "Peque√±os cambios en el dataset producen √°rboles **completamente diferentes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos base\n",
    "X, y = make_moons(n_samples=300, noise=0.25, random_state=RANDOM_STATE)\n",
    "\n",
    "# Funci√≥n para visualizar frontera\n",
    "\n",
    "\n",
    "def plot_decision_boundary(model, X, y, ax, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolors='k', alpha=0.8)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "# Entrenar 4 √°rboles en 4 subconjuntos diferentes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Muestreo bootstrap\n",
    "    idx = np.random.choice(len(X), size=len(X), replace=True)\n",
    "    X_boot, y_boot = X[idx], y[idx]\n",
    "\n",
    "    tree = DecisionTreeClassifier(max_depth=5, random_state=i)\n",
    "    tree.fit(X_boot, y_boot)\n",
    "\n",
    "    plot_decision_boundary(tree, X, y, ax, f'√Årbol #{i+1} (Bootstrap)')\n",
    "\n",
    "plt.suptitle('‚ö†Ô∏è 4 √Årboles, 4 Fronteras MUY Diferentes', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç Observa: Cada √°rbol captura patrones diferentes.\")\n",
    "print(\"   Si promediamos sus predicciones, el 'ruido' se cancela.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5c72b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. La Magia del Bagging\n",
    "\n",
    "### Bootstrap Aggregating (Bagging)\n",
    "\n",
    "```\n",
    "Para cada √°rbol t = 1, 2, ..., T:\n",
    "    1. Muestrear N ejemplos CON reemplazo (bootstrap)\n",
    "    2. Entrenar √°rbol en ese subconjunto\n",
    "    3. En cada split, usar ‚àöp features aleatorios\n",
    "\n",
    "Predicci√≥n final = VOTACI√ìN mayoritaria (clasificaci√≥n)\n",
    "                   PROMEDIO (regresi√≥n)\n",
    "```\n",
    "\n",
    "### üìê ¬øPor Qu√© Funciona?\n",
    "\n",
    "| Concepto | Explicaci√≥n |\n",
    "|----------|-------------|\n",
    "| **Diversidad** | Cada √°rbol ve datos y features diferentes |\n",
    "| **Decorrelaci√≥n** | Los errores de cada √°rbol son independientes |\n",
    "| **Reducci√≥n Varianza** | $\\sigma^2_{promedio} = \\frac{\\sigma^2}{n}$ |\n",
    "\n",
    "> **üí° Analog√≠a:** Es como preguntar direcciones a 100 personas aleatorias. Algunas se equivocar√°n, pero el promedio te llevar√° al destino."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb6c4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Random Forest en Acci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cfd42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE)\n",
    "\n",
    "# Entrenar Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=5, random_state=RANDOM_STATE)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Entrenar √°rbol simple para comparaci√≥n\n",
    "tree_simple = DecisionTreeClassifier(max_depth=5, random_state=RANDOM_STATE)\n",
    "tree_simple.fit(X_train, y_train)\n",
    "\n",
    "# Comparar fronteras\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "plot_decision_boundary(tree_simple, X, y, axes[0],\n",
    "                       f'Un Solo √Årbol\\nTest Acc: {tree_simple.score(X_test, y_test):.2%}')\n",
    "\n",
    "plot_decision_boundary(rf, X, y, axes[1],\n",
    "                       f'Random Forest (100 √°rboles)\\nTest Acc: {rf.score(X_test, y_test):.2%}')\n",
    "\n",
    "plt.suptitle('√Årbol Individual vs Random Forest', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar estabilidad con Cross-Validation\n",
    "tree_scores = cross_val_score(DecisionTreeClassifier(max_depth=5, random_state=RANDOM_STATE),\n",
    "                              X, y, cv=10)\n",
    "rf_scores = cross_val_score(RandomForestClassifier(n_estimators=100, max_depth=5, random_state=RANDOM_STATE),\n",
    "                            X, y, cv=10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "positions = [1, 2]\n",
    "bp = ax.boxplot([tree_scores, rf_scores], positions=positions,\n",
    "                widths=0.6, patch_artist=True)\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(['√Årbol Individual', 'Random Forest'])\n",
    "ax.set_ylabel('Accuracy (10-Fold CV)')\n",
    "ax.set_title('Variabilidad en Rendimiento: Un √Årbol vs 100 √Årboles')\n",
    "ax.axhline(y=np.mean(rf_scores), color='green', linestyle='--',\n",
    "           label=f'RF Media: {np.mean(rf_scores):.3f}')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"üìä √Årbol Individual: {np.mean(tree_scores):.3f} ¬± {np.std(tree_scores):.3f}\")\n",
    "print(\n",
    "    f\"üìä Random Forest:    {np.mean(rf_scores):.3f} ¬± {np.std(rf_scores):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee922731",
   "metadata": {},
   "source": [
    "> **üí° Pro-Tip: Estabilidad > Rendimiento Puntual**\n",
    "> Un modelo que da 85% consistentemente es mejor que uno que da 80%-95% seg√∫n el d√≠a. Random Forest brilla en estabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cec4283",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Importancia de Variables\n",
    "\n",
    "Una ventaja de Random Forest: **Feature Importance** basada en cu√°nto reduce cada variable la impureza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984353ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos con m√°s features\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_multi, y_multi = make_classification(\n",
    "    n_samples=500, n_features=10, n_informative=5, n_redundant=2,\n",
    "    n_clusters_per_class=2, random_state=RANDOM_STATE)\n",
    "\n",
    "feature_names = [f'Feature_{i}' for i in range(10)]\n",
    "\n",
    "# Entrenar RF\n",
    "rf_multi = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "rf_multi.fit(X_multi, y_multi)\n",
    "\n",
    "# Plot importancia\n",
    "importances = rf_multi.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(10), importances[indices], color='steelblue', alpha=0.8)\n",
    "plt.xticks(range(10), [feature_names[i] for i in indices], rotation=45)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importancia (Reducci√≥n Gini)')\n",
    "plt.title('üìä Feature Importance - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç Las primeras 5 features fueron creadas como 'informativas'.\")\n",
    "print(\"   El modelo las identifica correctamente como las m√°s importantes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b12efc",
   "metadata": {},
   "source": [
    "> **‚ö†Ô∏è Real-World Warning: Correlaci√≥n ‚â† Importancia Causal**\n",
    "> Feature importance mide asociaci√≥n predictiva, NO causalidad. Una variable proxy (correlacionada con el target) puede parecer importante aunque no cause el efecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c6637",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Hiperpar√°metros Clave\n",
    "\n",
    "| Par√°metro | Descripci√≥n | Valor T√≠pico |\n",
    "|-----------|-------------|---------------|\n",
    "| `n_estimators` | N√∫mero de √°rboles | 100-500 (m√°s = m√°s estable) |\n",
    "| `max_depth` | Profundidad de cada √°rbol | 5-15 o None |\n",
    "| `max_features` | Features por split | `sqrt` (clasificaci√≥n), `log2` |\n",
    "| `min_samples_leaf` | M√≠nimo en hojas | 1-5 |\n",
    "| `n_jobs` | Paralelizaci√≥n | -1 (todos los cores) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea35083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efecto del n√∫mero de √°rboles\n",
    "n_trees_list = [1, 5, 10, 25, 50, 100, 200, 500]\n",
    "scores = []\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE)\n",
    "\n",
    "for n_trees in n_trees_list:\n",
    "    rf_temp = RandomForestClassifier(\n",
    "        n_estimators=n_trees, max_depth=5, random_state=RANDOM_STATE)\n",
    "    rf_temp.fit(X_tr, y_tr)\n",
    "    scores.append(rf_temp.score(X_ts, y_ts))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(n_trees_list, scores, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('N√∫mero de √Årboles (n_estimators)')\n",
    "plt.ylabel('Accuracy en Test')\n",
    "plt.title('¬øCu√°ntos √Årboles Necesitamos?')\n",
    "plt.axhline(y=max(scores), color='green', linestyle='--', alpha=0.7)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Despu√©s de ~100 √°rboles, las ganancias son marginales.\")\n",
    "print(\"   M√°s √°rboles = m√°s tiempo de entrenamiento.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a6e33",
   "metadata": {},
   "source": [
    "> **üí° Pro-Tip: La Regla del 100**\n",
    "> Para la mayor√≠a de problemas, 100 √°rboles son suficientes. Solo aumenta si tienes datos muy ruidosos o alta dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14eb941",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Caso de Negocio: Predicci√≥n de Churn\n",
    "\n",
    "Usaremos datos de Telco para predecir qu√© clientes abandonar√°n el servicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0529e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos (asumimos que existe telco_churn.csv)\n",
    "try:\n",
    "    df = pd.read_csv('../data/telco_churn.csv')\n",
    "    print(f\"‚úÖ Dataset cargado: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Archivo no encontrado. Generando datos sint√©ticos para demostraci√≥n...\")\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n = 1000\n",
    "    df = pd.DataFrame({\n",
    "        'tenure': np.random.randint(1, 72, n),\n",
    "        'MonthlyCharges': np.random.uniform(20, 100, n),\n",
    "        'TotalCharges': np.random.uniform(100, 5000, n),\n",
    "        'Contract_Month': np.random.binomial(1, 0.5, n),\n",
    "        'PaymentMethod_Electronic': np.random.binomial(1, 0.4, n),\n",
    "        'InternetService_Fiber': np.random.binomial(1, 0.35, n),\n",
    "        'OnlineSecurity_No': np.random.binomial(1, 0.5, n),\n",
    "        'TechSupport_No': np.random.binomial(1, 0.5, n),\n",
    "    })\n",
    "    # Simular Churn basado en features\n",
    "    prob_churn = 0.1 + 0.3*(df['Contract_Month']) + 0.1 * \\\n",
    "        (df['tenure'] < 12) + 0.15*(df['OnlineSecurity_No'])\n",
    "    df['Churn'] = (np.random.rand(n) < prob_churn).astype(int)\n",
    "    print(f\"‚úÖ Datos sint√©ticos generados: {df.shape[0]} filas\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6210db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "target = 'Churn' if 'Churn' in df.columns else df.columns[-1]\n",
    "features = [col for col in df.columns if col !=\n",
    "            target and df[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "X_churn = df[features]\n",
    "y_churn = df[target]\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X_churn, y_churn, test_size=0.3,\n",
    "                                          stratify=y_churn, random_state=RANDOM_STATE)\n",
    "\n",
    "# Entrenar Random Forest\n",
    "rf_churn = RandomForestClassifier(n_estimators=100, max_depth=6,\n",
    "                                  min_samples_leaf=10, random_state=RANDOM_STATE)\n",
    "rf_churn.fit(X_tr, y_tr)\n",
    "\n",
    "print(f\"üìä Accuracy en Test: {rf_churn.score(X_ts, y_ts):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9622fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance para Churn\n",
    "importances = rf_churn.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(features)), importances[indices], color='coral')\n",
    "plt.yticks(range(len(features)), [features[i] for i in indices])\n",
    "plt.xlabel('Importancia (Reducci√≥n Gini)')\n",
    "plt.title('üîç ¬øQu√© Variables Predicen Churn?')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c13ea",
   "metadata": {},
   "source": [
    "### üß† Micro-Desaf√≠o: Interpretando Feature Importance\n",
    "\n",
    "**Pregunta:** Seg√∫n el gr√°fico, ¬øcu√°les son las 3 variables m√°s importantes para predecir churn?\n",
    "\n",
    "**Reflexi√≥n:** ¬øQu√© acciones de negocio podr√≠as recomendar bas√°ndote en estas variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccbc5dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Resumen y Siguiente Paso\n",
    "\n",
    "### üèÜ Resumen de Logros\n",
    "¬°Felicidades! En este notebook has aprendido:\n",
    "\n",
    "1. **Bagging:** Entrenar m√∫ltiples modelos en subconjuntos bootstrap.\n",
    "2. **Decorrelaci√≥n:** La aleatoriedad en features reduce la correlaci√≥n entre √°rboles.\n",
    "3. **Reducci√≥n de Varianza:** El promedio de muchos √°rboles es m√°s estable que uno solo.\n",
    "4. **Feature Importance:** Identificar qu√© variables son m√°s predictivas.\n",
    "5. **Paralelizaci√≥n:** Los √°rboles se entrenan independientemente (escalabilidad).\n",
    "\n",
    "### ‚úÖ Ventajas de Random Forest\n",
    "- Robusto a outliers y ruido\n",
    "- Poco preprocesamiento requerido\n",
    "- Maneja bien desbalance de clases (con `class_weight`)\n",
    "- Feature importance interpretable\n",
    "\n",
    "### ‚ö†Ô∏è Limitaciones\n",
    "- Menos interpretable que un √°rbol solo\n",
    "- Puede ser lento con millones de registros\n",
    "- No extrapola bien fuera del rango de datos\n",
    "\n",
    "---\n",
    "\n",
    "### üëâ Siguiente Paso\n",
    "Random Forest promedia √°rboles independientes (paralelos). ¬øY si cada √°rbol **aprendiera de los errores del anterior**?\n",
    "\n",
    "**Gradient Boosting:** Entrenar √°rboles secuencialmente, donde cada uno corrige los errores del anterior.\n",
    "\n",
    "*En el siguiente notebook veremos XGBoost y LightGBM, los reyes de Kaggle.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
