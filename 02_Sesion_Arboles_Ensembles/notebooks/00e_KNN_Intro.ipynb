{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Imports y Configuraci√≥n\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_moons, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Configuraci√≥n Visual\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa52838c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. La Intuici√≥n: Dime con Qui√©n Andas...\n",
    "\n",
    "### El Algoritmo M√°s Simple Posible\n",
    "\n",
    "```\n",
    "Para clasificar un nuevo punto X:\n",
    "1. Calcular la distancia de X a TODOS los puntos de entrenamiento\n",
    "2. Encontrar los K puntos m√°s cercanos (vecinos)\n",
    "3. Votaci√≥n: La clase mayoritaria entre los K vecinos gana\n",
    "```\n",
    "\n",
    "### Ejemplo Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos simples\n",
    "np.random.seed(RANDOM_STATE)\n",
    "X_simple = np.vstack([np.random.randn(15, 2) + [2, 2],\n",
    "                      np.random.randn(15, 2) + [-1, -1]])\n",
    "y_simple = np.array([0]*15 + [1]*15)\n",
    "\n",
    "# Punto nuevo a clasificar\n",
    "nuevo_punto = np.array([[0.5, 1]])\n",
    "\n",
    "# Calcular distancias\n",
    "distancias = np.sqrt(((X_simple - nuevo_punto)**2).sum(axis=1))\n",
    "\n",
    "# Encontrar K=5 vecinos m√°s cercanos\n",
    "K = 5\n",
    "indices_cercanos = np.argsort(distancias)[:K]\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_simple[y_simple == 0, 0], X_simple[y_simple == 0, 1],\n",
    "            c='blue', label='Clase 0', s=100, alpha=0.6)\n",
    "plt.scatter(X_simple[y_simple == 1, 0], X_simple[y_simple == 1, 1],\n",
    "            c='red', label='Clase 1', s=100, alpha=0.6)\n",
    "\n",
    "# Punto nuevo\n",
    "plt.scatter(nuevo_punto[0, 0], nuevo_punto[0, 1],\n",
    "            c='green', marker='*', s=500, label='Nuevo Punto', edgecolors='k', linewidth=2)\n",
    "\n",
    "# Dibujar l√≠neas a los K vecinos\n",
    "for idx in indices_cercanos:\n",
    "    plt.plot([nuevo_punto[0, 0], X_simple[idx, 0]],\n",
    "             [nuevo_punto[0, 1], X_simple[idx, 1]],\n",
    "             'g--', alpha=0.5)\n",
    "    plt.scatter(X_simple[idx, 0], X_simple[idx, 1],\n",
    "                s=300, facecolors='none', edgecolors='green', linewidth=3)\n",
    "\n",
    "# Conteo de votos\n",
    "votos = y_simple[indices_cercanos]\n",
    "prediccion = np.bincount(votos).argmax()\n",
    "\n",
    "plt.title(f'KNN con K={K}: Vecinos cercanos y votaci√≥n\\n'\n",
    "          f'Votos: Clase 0 = {sum(votos == 0)}, Clase 1 = {sum(votos == 1)} ‚Üí Predicci√≥n: Clase {prediccion}')\n",
    "plt.legend()\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7134a6",
   "metadata": {},
   "source": [
    "> **üí° Pro-Tip: KNN es un \"Lazy Learner\"**\n",
    "> No hay fase de entrenamiento. Todo el trabajo se hace en predicci√≥n. Esto significa:\n",
    "> - Entrenamiento: O(1) - Solo guardar datos\n",
    "> - Predicci√≥n: O(n¬∑d) - Calcular distancia a todos los n puntos en d dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44227522",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Eligiendo K: El Par√°metro Cr√≠tico\n",
    "\n",
    "### El Tradeoff de K\n",
    "\n",
    "| K | Comportamiento | Riesgo |\n",
    "|---|----------------|--------|\n",
    "| **K=1** | Clasifica igual que el vecino m√°s cercano | Alto overfitting, sensible a ruido |\n",
    "| **K peque√±o** | Frontera muy irregular | Alta varianza |\n",
    "| **K grande** | Frontera suave | Alto sesgo (puede ignorar patrones locales) |\n",
    "| **K=n** | Siempre predice la clase mayoritaria | Modelo in√∫til |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad1ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar efecto de K\n",
    "X_moons, y_moons = make_moons(\n",
    "    n_samples=200, noise=0.25, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def plot_knn_boundary(X, y, k, ax, title):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X, y)\n",
    "\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolors='k', alpha=0.8)\n",
    "    ax.set_title(title)\n",
    "    return knn.score(X, y)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "k_values = [1, 3, 5, 15, 50, 100]\n",
    "\n",
    "for ax, k in zip(axes.flat, k_values):\n",
    "    acc = plot_knn_boundary(X_moons, y_moons, k, ax, f'K = {k}')\n",
    "\n",
    "plt.suptitle('Efecto de K en la Frontera de Decisi√≥n', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç K=1: Frontera muy dentada (overfitting)\")\n",
    "print(\"üîç K grande: Frontera demasiado suave (underfitting)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e16500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar el mejor K con Cross-Validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.3, random_state=RANDOM_STATE)\n",
    "\n",
    "k_range = range(1, 31)\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_range, cv_scores, 'o-', linewidth=2, markersize=6)\n",
    "plt.xlabel('K (N√∫mero de Vecinos)')\n",
    "plt.ylabel('Accuracy (5-Fold CV)')\n",
    "plt.title('¬øCu√°l es el Mejor K?')\n",
    "plt.axvline(x=k_range[np.argmax(cv_scores)], color='red', linestyle='--',\n",
    "            label=f'Mejor K = {k_range[np.argmax(cv_scores)]}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Mejor K: {k_range[np.argmax(cv_scores)]}\")\n",
    "print(f\"üìä Mejor CV Score: {max(cv_scores):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced7cee",
   "metadata": {},
   "source": [
    "> **üí° Pro-Tip: K Impar**\n",
    "> Para clasificaci√≥n binaria, usa K impar para evitar empates en la votaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d301b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. El Efecto de la Distancia\n",
    "\n",
    "### M√©tricas de Distancia\n",
    "\n",
    "| M√©trica | F√≥rmula | Uso |\n",
    "|---------|---------|-----|\n",
    "| **Euclidiana (L2)** | $\\sqrt{\\sum(x_i - y_i)^2}$ | Default, distancia \"en l√≠nea recta\" |\n",
    "| **Manhattan (L1)** | $\\sum|x_i - y_i|$ | Cuando features tienen escalas diferentes |\n",
    "| **Minkowski** | $(\\sum|x_i - y_i|^p)^{1/p}$ | Generalizaci√≥n (p=1 Manhattan, p=2 Euclidiana) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e324fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar m√©tricas de distancia\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "metrics = ['euclidean', 'manhattan', 'chebyshev']\n",
    "titles = ['Euclidiana (L2)', 'Manhattan (L1)', 'Chebyshev (L‚àû)']\n",
    "\n",
    "for ax, metric, title in zip(axes, metrics, titles):\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
    "    knn.fit(X_moons, y_moons)\n",
    "\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_moons[:, 0].min() - 0.5, X_moons[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_moons[:, 1].min() - 0.5, X_moons[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')\n",
    "    ax.scatter(X_moons[:, 0], X_moons[:, 1],\n",
    "               c=y_moons, cmap='RdBu', edgecolors='k')\n",
    "    ax.set_title(f'{title}\\nAcc: {knn.score(X_moons, y_moons):.2%}')\n",
    "\n",
    "plt.suptitle('Efecto de la M√©trica de Distancia (K=5)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7917701b",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è La Importancia del Escalado\n",
    "\n",
    "Si una feature va de 0 a 1 y otra de 0 a 1,000,000, la segunda dominar√° la distancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c4292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n del problema de escala\n",
    "np.random.seed(RANDOM_STATE)\n",
    "X_scale_problem = X_moons.copy()\n",
    "X_scale_problem[:, 1] = X_scale_problem[:, 1] * \\\n",
    "    1000  # Segunda feature escalada x1000\n",
    "\n",
    "# Sin escalar\n",
    "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
    "scores_no_scale = cross_val_score(knn_no_scale, X_scale_problem, y_moons, cv=5)\n",
    "\n",
    "# Con escalar\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_scale_problem)\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "scores_scaled = cross_val_score(knn_scaled, X_scaled, y_moons, cv=5)\n",
    "\n",
    "print(\n",
    "    f\"üìä Sin escalar: {np.mean(scores_no_scale):.2%} ¬± {np.std(scores_no_scale):.2%}\")\n",
    "print(\n",
    "    f\"üìä Con escalar: {np.mean(scores_scaled):.2%} ¬± {np.std(scores_scaled):.2%}\")\n",
    "print(\"\\n‚ö†Ô∏è ¬°El escalado es CR√çTICO para KNN!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea42153a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. KNN en Acci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar KNN con otros algoritmos\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Preparar datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_moons)\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(\n",
    "    X_scaled, y_moons, test_size=0.3, random_state=RANDOM_STATE)\n",
    "\n",
    "# Modelos\n",
    "models = {\n",
    "    'KNN (K=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=RANDOM_STATE),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_tr, y_tr)\n",
    "    train_acc = model.score(X_tr, y_tr)\n",
    "    test_acc = model.score(X_ts, y_ts)\n",
    "    results.append(\n",
    "        {'Modelo': name, 'Train Acc': train_acc, 'Test Acc': test_acc})\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a02d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar fronteras de todos los modelos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, (name, model) in zip(axes.flat, models.items()):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_scaled[:, 0].min() - 0.5, X_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_scaled[:, 1].min() - 0.5, X_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')\n",
    "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1],\n",
    "               c=y_moons, cmap='RdBu', edgecolors='k')\n",
    "    ax.set_title(f'{name}\\nTest: {model.score(X_ts, y_ts):.2%}')\n",
    "\n",
    "plt.suptitle('Comparaci√≥n de Fronteras de Decisi√≥n', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b53579",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. La Maldici√≥n de la Dimensionalidad\n",
    "\n",
    "### ¬øPor Qu√© KNN Falla en Alta Dimensi√≥n?\n",
    "\n",
    "En espacios de alta dimensi√≥n:\n",
    "1. **Todos los puntos est√°n \"lejos\"** unos de otros\n",
    "2. El concepto de \"vecino m√°s cercano\" pierde significado\n",
    "3. Se necesitan exponencialmente m√°s datos para cubrir el espacio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d1714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n: KNN vs dimensionalidad\n",
    "dimensions = [2, 5, 10, 20, 50, 100]\n",
    "results_dim = []\n",
    "\n",
    "for d in dimensions:\n",
    "    # Generar datos en d dimensiones\n",
    "    X_d, y_d = make_classification(n_samples=500, n_features=d, n_informative=d//2,\n",
    "                                   n_redundant=0, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Escalar\n",
    "    X_d_scaled = StandardScaler().fit_transform(X_d)\n",
    "\n",
    "    # Cross-validation\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    scores = cross_val_score(knn, X_d_scaled, y_d, cv=5)\n",
    "\n",
    "    results_dim.append({\n",
    "        'Dimensiones': d,\n",
    "        'Accuracy': np.mean(scores),\n",
    "        'Std': np.std(scores)\n",
    "    })\n",
    "\n",
    "df_dim = pd.DataFrame(results_dim)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(df_dim['Dimensiones'], df_dim['Accuracy'],\n",
    "             yerr=df_dim['Std'], marker='o', linewidth=2, capsize=5)\n",
    "plt.xlabel('N√∫mero de Dimensiones')\n",
    "plt.ylabel('Accuracy (5-Fold CV)')\n",
    "plt.title('La Maldici√≥n de la Dimensionalidad: KNN Degrada con M√°s Features')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìâ A medida que aumentan las dimensiones, KNN pierde efectividad.\")\n",
    "print(\"üí° Soluci√≥n: Reducci√≥n de dimensionalidad (PCA) antes de KNN.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725f64e7",
   "metadata": {},
   "source": [
    "> **‚ö†Ô∏è Real-World Warning: KNN en Producci√≥n**\n",
    "> - **Memoria:** Almacena TODOS los datos de entrenamiento\n",
    "> - **Latencia:** Cada predicci√≥n recorre todos los puntos\n",
    "> - **Soluci√≥n para escala:** Usar estructuras como KD-Tree o Ball-Tree (sklearn lo hace autom√°tico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b10bd2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Caso de Negocio: Sistema de Recomendaci√≥n Simple\n",
    "\n",
    "KNN es la base de muchos sistemas de recomendaci√≥n \"Collaborative Filtering\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b70b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular datos de usuarios y productos\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# 100 usuarios, 20 productos (ratings 1-5 o NaN)\n",
    "n_users = 100\n",
    "n_products = 20\n",
    "\n",
    "# Crear perfiles de usuario (features latentes)\n",
    "user_profiles = np.random.rand(n_users, 5)  # 5 features latentes\n",
    "product_profiles = np.random.rand(n_products, 5)\n",
    "\n",
    "# Ratings = producto punto entre perfiles (+ ruido)\n",
    "ratings = np.dot(user_profiles, product_profiles.T) * 4 + 1  # Escala 1-5\n",
    "ratings += np.random.randn(n_users, n_products) * 0.5\n",
    "ratings = np.clip(ratings, 1, 5).round(1)\n",
    "\n",
    "# Simular ratings faltantes (70% observados)\n",
    "mask = np.random.rand(n_users, n_products) < 0.7\n",
    "ratings_sparse = np.where(mask, ratings, np.nan)\n",
    "\n",
    "df_ratings = pd.DataFrame(ratings_sparse,\n",
    "                          index=[f'User_{i}' for i in range(n_users)],\n",
    "                          columns=[f'Prod_{j}' for j in range(n_products)])\n",
    "\n",
    "print(\"üìä Matriz de Ratings (con NaN = no visto):\")\n",
    "df_ratings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6783f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomendar para User_0 bas√°ndonos en usuarios similares\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Imputar valores faltantes con la media (para calcular similitud)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "ratings_imputed = imputer.fit_transform(ratings_sparse)\n",
    "\n",
    "# Encontrar 5 usuarios similares a User_0\n",
    "# 6 porque incluye a s√≠ mismo\n",
    "knn_rec = NearestNeighbors(n_neighbors=6, metric='cosine')\n",
    "knn_rec.fit(ratings_imputed)\n",
    "\n",
    "distances, indices = knn_rec.kneighbors([ratings_imputed[0]])\n",
    "similar_users = indices[0][1:]  # Excluir User_0\n",
    "\n",
    "print(f\"üîç Usuarios similares a User_0: {[f'User_{i}' for i in similar_users]}\")\n",
    "\n",
    "# Recomendar productos que User_0 no ha visto pero sus vecinos s√≠\n",
    "productos_no_vistos = np.where(np.isnan(ratings_sparse[0]))[0]\n",
    "\n",
    "if len(productos_no_vistos) > 0:\n",
    "    # Rating promedio de vecinos para productos no vistos\n",
    "    pred_ratings = []\n",
    "    for prod in productos_no_vistos:\n",
    "        vecinos_ratings = ratings_imputed[similar_users, prod]\n",
    "        pred_ratings.append((f'Prod_{prod}', np.mean(vecinos_ratings)))\n",
    "\n",
    "    # Ordenar por rating predicho\n",
    "    pred_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\nüéØ Top 5 Recomendaciones para User_0:\")\n",
    "    for prod, rating in pred_ratings[:5]:\n",
    "        print(f\"   {prod}: Rating predicho = {rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024bacf6",
   "metadata": {},
   "source": [
    "### üß† Micro-Desaf√≠o: Limitaciones del Enfoque\n",
    "\n",
    "**Reflexiona:**\n",
    "1. ¬øQu√© pasa con usuarios nuevos (sin ratings)? ‚Üí **Cold Start Problem**\n",
    "2. ¬øQu√© pasa si un usuario tiene gustos muy √∫nicos? ‚Üí **Sparsity Problem**\n",
    "3. ¬øEscala este enfoque a millones de usuarios? ‚Üí **Scalability Problem**\n",
    "\n",
    "**Soluciones modernas:** Matrix Factorization (SVD), Deep Learning (Neural Collaborative Filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e037c9a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Resumen y Siguiente Paso\n",
    "\n",
    "### üèÜ Resumen de Logros\n",
    "¬°Felicidades! En este notebook has aprendido:\n",
    "\n",
    "1. **Lazy Learning:** KNN no entrena, solo memoriza y busca en predicci√≥n.\n",
    "2. **Elecci√≥n de K:** Balance entre overfitting (K bajo) y underfitting (K alto).\n",
    "3. **M√©tricas de Distancia:** Euclidiana, Manhattan, y por qu√© importan.\n",
    "4. **Escalado:** CR√çTICO para que las distancias tengan sentido.\n",
    "5. **Maldici√≥n de la Dimensionalidad:** KNN sufre en alta dimensi√≥n.\n",
    "\n",
    "### ‚úÖ Cu√°ndo Usar KNN\n",
    "- Datasets peque√±os/medianos (<50k muestras)\n",
    "- Pocas dimensiones (<20 features)\n",
    "- Cuando necesitas explicabilidad (\"se parece a estos ejemplos\")\n",
    "- Sistemas de recomendaci√≥n simples\n",
    "\n",
    "### ‚ö†Ô∏è Cu√°ndo Evitar KNN\n",
    "- Datasets grandes (predicci√≥n lenta)\n",
    "- Alta dimensionalidad\n",
    "- Cuando la memoria es limitada\n",
    "\n",
    "---\n",
    "\n",
    "### üëâ Siguiente Paso\n",
    "¬°Has completado todos los algoritmos introductorios de la Sesi√≥n 2!\n",
    "\n",
    "**Resumen de Modelos Cubiertos:**\n",
    "- üå≥ √Årboles de Decisi√≥n (CART)\n",
    "- üå≤ Random Forest (Bagging)\n",
    "- üöÄ Gradient Boosting (XGBoost, LightGBM)\n",
    "- üìê Support Vector Machines (SVM)\n",
    "- üîç K-Nearest Neighbors (KNN)\n",
    "\n",
    "*En el notebook pr√°ctico `01_Algoritmos_No_Lineales.ipynb` aplicar√°s todos estos algoritmos a un dataset real de Churn.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
