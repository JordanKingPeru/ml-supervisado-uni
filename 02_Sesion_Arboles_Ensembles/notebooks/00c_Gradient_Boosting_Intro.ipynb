{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b65c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Imports y Configuraci√≥n\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import make_moons, make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-Learn\n",
    "\n",
    "# Boosting Libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost no instalado. Ejecuta: pip install xgboost\")\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LightGBM no instalado. Ejecuta: pip install lightgbm\")\n",
    "    LGBM_AVAILABLE = False\n",
    "\n",
    "# Configuraci√≥n Visual\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fd2e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Del Bagging al Boosting\n",
    "\n",
    "### Comparaci√≥n Visual\n",
    "\n",
    "| Aspecto | Random Forest (Bagging) | Gradient Boosting |\n",
    "|---------|-------------------------|-------------------|\n",
    "| **Entrenamiento** | Paralelo | Secuencial |\n",
    "| **Cada √Årbol Aprende** | Del dataset completo | De los errores anteriores |\n",
    "| **Combinaci√≥n** | Votaci√≥n/Promedio | Suma ponderada |\n",
    "| **Reduce** | Varianza | Sesgo |\n",
    "| **Riesgo** | Bajo | Overfitting si no controlas |\n",
    "\n",
    "```\n",
    "Random Forest:    üå≥ + üå≥ + üå≥ + ... ‚Üí PROMEDIO\n",
    "                  (cada uno independiente)\n",
    "\n",
    "Gradient Boosting: üå± ‚Üí üå± ‚Üí üå± ‚Üí ... ‚Üí SUMA\n",
    "                   (cada uno corrige al anterior)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cfbe15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. La Intuici√≥n: Aprender del Error\n",
    "\n",
    "### El Algoritmo (Simplificado)\n",
    "\n",
    "1. **Inicializar:** Predicci√≥n base (ej: promedio del target)\n",
    "2. **Para cada iteraci√≥n t:**\n",
    "   - Calcular residuales: $r_i = y_i - \\hat{y}_i$\n",
    "   - Entrenar √°rbol peque√±o para predecir residuales\n",
    "   - Actualizar: $\\hat{y} = \\hat{y} + \\eta \\cdot h_t(x)$\n",
    "3. **Predicci√≥n final:** Suma de todos los √°rboles\n",
    "\n",
    "### üîë La Clave: Learning Rate (Œ∑)\n",
    "\n",
    "$$\\hat{y}^{(t)} = \\hat{y}^{(t-1)} + \\eta \\cdot h_t(x)$$\n",
    "\n",
    "- **Œ∑ alto (0.3):** Aprende r√°pido, pero puede saltar el √≥ptimo\n",
    "- **Œ∑ bajo (0.01):** Aprende lento, pero m√°s preciso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n: C√≥mo el Boosting corrige errores iterativamente\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=0.25, random_state=RANDOM_STATE)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE)\n",
    "\n",
    "# Funci√≥n para visualizar frontera\n",
    "\n",
    "\n",
    "def plot_decision_boundary(model, X, y, ax, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolors='k', alpha=0.8)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "# Entrenar con diferentes n√∫meros de iteraciones\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "iterations = [1, 5, 20, 100]\n",
    "\n",
    "for ax, n_iter in zip(axes, iterations):\n",
    "    gb = GradientBoostingClassifier(n_estimators=n_iter, learning_rate=0.1,\n",
    "                                    max_depth=3, random_state=RANDOM_STATE)\n",
    "    gb.fit(X_train, y_train)\n",
    "    plot_decision_boundary(gb, X, y, ax,\n",
    "                           f'{n_iter} iteraciones\\nAcc: {gb.score(X_test, y_test):.2%}')\n",
    "\n",
    "plt.suptitle('Gradient Boosting: Correcci√≥n Iterativa de Errores', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d3376",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. XGBoost en Acci√≥n\n",
    "\n",
    "### ¬øPor Qu√© XGBoost Domina Kaggle?\n",
    "\n",
    "| Caracter√≠stica | Beneficio |\n",
    "|----------------|----------|\n",
    "| **Regularizaci√≥n L1/L2** | Controla overfitting |\n",
    "| **Manejo de Missing Values** | No requiere imputaci√≥n |\n",
    "| **Tree Pruning** | Poda inteligente (no greedy) |\n",
    "| **Paralelizaci√≥n** | R√°pido a pesar de ser secuencial |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if XGB_AVAILABLE:\n",
    "    # Entrenar XGBoost\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        random_state=RANDOM_STATE,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"üìä XGBoost Accuracy: {xgb_model.score(X_test, y_test):.2%}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è XGBoost no disponible. Usando GradientBoostingClassifier de sklearn.\")\n",
    "    xgb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "                                           max_depth=4, random_state=RANDOM_STATE)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    print(\n",
    "        f\"üìä GradientBoosting Accuracy: {xgb_model.score(X_test, y_test):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7bc376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva de aprendizaje: Training vs Validation\n",
    "if XGB_AVAILABLE:\n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_model_track = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        random_state=RANDOM_STATE,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_model_track.fit(X_train, y_train, eval_set=eval_set, verbose=False)\n",
    "\n",
    "    results = xgb_model_track.evals_result()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(results['validation_0']['logloss'], label='Train')\n",
    "    plt.plot(results['validation_1']['logloss'], label='Test')\n",
    "    plt.xlabel('Iteraciones (N√∫mero de √Årboles)')\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.title('Curva de Aprendizaje XGBoost')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚ö†Ô∏è Si Test sube mientras Train baja ‚Üí OVERFITTING\")\n",
    "    print(\"üí° Soluci√≥n: Early Stopping o reducir n_estimators\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c0cf2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. LightGBM: Velocidad y Escala\n",
    "\n",
    "### ¬øPor Qu√© LightGBM es M√°s R√°pido?\n",
    "\n",
    "| T√©cnica | Descripci√≥n |\n",
    "|---------|-------------|\n",
    "| **Leaf-wise Growth** | Crece el nodo que m√°s reduce el error (vs level-wise) |\n",
    "| **Histogram-based** | Agrupa valores continuos en bins |\n",
    "| **GOSS** | Muestrea inteligentemente (mantiene gradientes grandes) |\n",
    "\n",
    "```\n",
    "XGBoost (Level-wise):    [___________]\n",
    "                         [_____][_____]\n",
    "                         [__][__][__][__]\n",
    "\n",
    "LightGBM (Leaf-wise):    [___________]\n",
    "                         [_____][_____]\n",
    "                         [__]        [_____]\n",
    "                                     [__][__]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbbfb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LGBM_AVAILABLE:\n",
    "    # Entrenar LightGBM\n",
    "    lgbm_model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        num_leaves=15,  # Par√°metro clave en LightGBM\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"üìä LightGBM Accuracy: {lgbm_model.score(X_test, y_test):.2%}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è LightGBM no disponible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd1e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n de tiempo de entrenamiento (con dataset m√°s grande)\n",
    "import time\n",
    "\n",
    "X_large, y_large = make_classification(n_samples=10000, n_features=20,\n",
    "                                       n_informative=10, random_state=RANDOM_STATE)\n",
    "X_tr_l, X_ts_l, y_tr_l, y_ts_l = train_test_split(X_large, y_large, test_size=0.3,\n",
    "                                                  random_state=RANDOM_STATE)\n",
    "\n",
    "times = {}\n",
    "\n",
    "# Random Forest\n",
    "start = time.time()\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=6, random_state=RANDOM_STATE)\n",
    "rf.fit(X_tr_l, y_tr_l)\n",
    "times['Random Forest'] = time.time() - start\n",
    "\n",
    "# Gradient Boosting sklearn\n",
    "start = time.time()\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=100, max_depth=4, random_state=RANDOM_STATE)\n",
    "gb.fit(X_tr_l, y_tr_l)\n",
    "times['GradientBoosting'] = time.time() - start\n",
    "\n",
    "if XGB_AVAILABLE:\n",
    "    start = time.time()\n",
    "    xgb_l = xgb.XGBClassifier(n_estimators=100, max_depth=4, random_state=RANDOM_STATE,\n",
    "                              use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb_l.fit(X_tr_l, y_tr_l)\n",
    "    times['XGBoost'] = time.time() - start\n",
    "\n",
    "if LGBM_AVAILABLE:\n",
    "    start = time.time()\n",
    "    lgbm_l = lgb.LGBMClassifier(\n",
    "        n_estimators=100, max_depth=4, random_state=RANDOM_STATE, verbose=-1)\n",
    "    lgbm_l.fit(X_tr_l, y_tr_l)\n",
    "    times['LightGBM'] = time.time() - start\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(times.keys(), times.values(), color=[\n",
    "        '#4ECDC4', '#FFE66D', '#FF6B6B', '#95E1D3'])\n",
    "plt.ylabel('Tiempo (segundos)')\n",
    "plt.title('‚è±Ô∏è Tiempo de Entrenamiento (10,000 muestras, 100 √°rboles)')\n",
    "for i, (k, v) in enumerate(times.items()):\n",
    "    plt.text(i, v + 0.05, f'{v:.2f}s', ha='center')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e25a295",
   "metadata": {},
   "source": [
    "> **üí° Pro-Tip: ¬øCu√°ndo Usar Cada Uno?**\n",
    "> - **Random Forest:** Baseline r√°pido, pocos hiperpar√°metros que tunear.\n",
    "> - **XGBoost:** Competencias, m√°ximo rendimiento con tuning.\n",
    "> - **LightGBM:** Datasets grandes (>100k filas), categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3f02a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Hiperpar√°metros Cr√≠ticos\n",
    "\n",
    "### Par√°metros Comunes (XGBoost/LightGBM)\n",
    "\n",
    "| Par√°metro | XGBoost | LightGBM | Efecto |\n",
    "|-----------|---------|----------|--------|\n",
    "| √Årboles | `n_estimators` | `n_estimators` | M√°s = m√°s potencia (y overfitting) |\n",
    "| Learning Rate | `learning_rate` | `learning_rate` | Menor = m√°s estable |\n",
    "| Profundidad | `max_depth` | `max_depth` | Control de complejidad |\n",
    "| Hojas | N/A | `num_leaves` | Control principal en LGBM |\n",
    "| Regularizaci√≥n | `reg_alpha`, `reg_lambda` | `reg_alpha`, `reg_lambda` | Penalizar complejidad |\n",
    "\n",
    "### üéØ Receta Pr√°ctica\n",
    "\n",
    "```python\n",
    "# Configuraci√≥n inicial conservadora\n",
    "params = {\n",
    "    'n_estimators': 500,       # Alto, pero con early stopping\n",
    "    'learning_rate': 0.05,     # Bajo para estabilidad\n",
    "    'max_depth': 4,            # √Årboles poco profundos\n",
    "    'early_stopping_rounds': 50,  # Parar si no mejora\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0fe5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Efecto del Learning Rate\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.3]\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    if XGB_AVAILABLE:\n",
    "        model = xgb.XGBClassifier(n_estimators=200, learning_rate=lr, max_depth=4,\n",
    "                                  random_state=RANDOM_STATE, use_label_encoder=False,\n",
    "                                  eval_metric='logloss')\n",
    "    else:\n",
    "        model = GradientBoostingClassifier(n_estimators=200, learning_rate=lr,\n",
    "                                           max_depth=4, random_state=RANDOM_STATE)\n",
    "\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    results.append({\n",
    "        'Learning Rate': lr,\n",
    "        'Mean Accuracy': np.mean(scores),\n",
    "        'Std': np.std(scores)\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319bf520",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Caso de Negocio: Detecci√≥n de Fraude\n",
    "\n",
    "Simularemos un dataset de detecci√≥n de fraude (muy desbalanceado, como en la vida real)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01555526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular datos de fraude (1% fraude, 99% leg√≠timo)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "n = 5000\n",
    "\n",
    "# Features\n",
    "monto = np.random.exponential(100, n)\n",
    "hora = np.random.randint(0, 24, n)\n",
    "distancia_casa = np.random.exponential(50, n)\n",
    "es_fin_semana = np.random.binomial(1, 2/7, n)\n",
    "transacciones_24h = np.random.poisson(3, n)\n",
    "\n",
    "# Target (fraude m√°s probable si: monto alto, hora nocturna, lejos de casa)\n",
    "prob_fraude = 0.005 + 0.002*(monto > 200) + 0.003*(hora < 6) + \\\n",
    "    0.002*(distancia_casa > 100) + 0.003*(transacciones_24h > 5)\n",
    "fraude = (np.random.rand(n) < prob_fraude).astype(int)\n",
    "\n",
    "df_fraude = pd.DataFrame({\n",
    "    'Monto': monto,\n",
    "    'Hora': hora,\n",
    "    'Distancia_Casa': distancia_casa,\n",
    "    'Fin_Semana': es_fin_semana,\n",
    "    'Transacciones_24h': transacciones_24h,\n",
    "    'Fraude': fraude\n",
    "})\n",
    "\n",
    "print(f\"üìä Distribuci√≥n de clases:\")\n",
    "print(df_fraude['Fraude'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "features = ['Monto', 'Hora', 'Distancia_Casa',\n",
    "            'Fin_Semana', 'Transacciones_24h']\n",
    "X_fraud = df_fraude[features]\n",
    "y_fraud = df_fraude['Fraude']\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X_fraud, y_fraud, test_size=0.3,\n",
    "                                          stratify=y_fraud, random_state=RANDOM_STATE)\n",
    "\n",
    "# Entrenar modelos\n",
    "rf_fraud = RandomForestClassifier(n_estimators=100, max_depth=5,\n",
    "                                  class_weight='balanced', random_state=RANDOM_STATE)\n",
    "rf_fraud.fit(X_tr, y_tr)\n",
    "\n",
    "if LGBM_AVAILABLE:\n",
    "    lgbm_fraud = lgb.LGBMClassifier(n_estimators=100, max_depth=5,\n",
    "                                    class_weight='balanced', random_state=RANDOM_STATE, verbose=-1)\n",
    "    lgbm_fraud.fit(X_tr, y_tr)\n",
    "\n",
    "# Evaluar\n",
    "print(\"=\"*50)\n",
    "print(\"üìä EVALUACI√ìN EN DETECCI√ìN DE FRAUDE\")\n",
    "print(\"=\"*50)\n",
    "print(\n",
    "    f\"Random Forest ROC-AUC: {roc_auc_score(y_ts, rf_fraud.predict_proba(X_ts)[:, 1]):.3f}\")\n",
    "if LGBM_AVAILABLE:\n",
    "    print(\n",
    "        f\"LightGBM ROC-AUC:      {roc_auc_score(y_ts, lgbm_fraud.predict_proba(X_ts)[:, 1]):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f43bb",
   "metadata": {},
   "source": [
    "> **‚ö†Ô∏è Real-World Warning: Clases Desbalanceadas**\n",
    "> En fraude, el desbalance es extremo (0.1% fraude). Siempre usa:\n",
    "> - `class_weight='balanced'`\n",
    "> - M√©tricas como ROC-AUC, Precision-Recall (NO solo Accuracy)\n",
    "> - T√©cnicas de resampling (SMOTE) si es necesario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc23ab",
   "metadata": {},
   "source": [
    "### üß† Micro-Desaf√≠o: Early Stopping\n",
    "\n",
    "**Pregunta:** Si entrenas un modelo con 1000 √°rboles y `learning_rate=0.01`, ¬øc√≥mo sabes cu√°ndo parar?\n",
    "\n",
    "**Respuesta:** Usa `early_stopping_rounds` para detener el entrenamiento cuando el error de validaci√≥n no mejora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1907dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Early Stopping\n",
    "if XGB_AVAILABLE:\n",
    "    xgb_early = xgb.XGBClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=4,\n",
    "        early_stopping_rounds=50,\n",
    "        random_state=RANDOM_STATE,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc'\n",
    "    )\n",
    "    xgb_early.fit(X_tr, y_tr, eval_set=[(X_ts, y_ts)], verbose=False)\n",
    "\n",
    "    print(\n",
    "        f\"üõë Early Stopping: Entren√≥ {xgb_early.best_iteration} √°rboles (de 1000 posibles)\")\n",
    "    print(f\"üìä Best ROC-AUC: {xgb_early.best_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebedac46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Resumen y Siguiente Paso\n",
    "\n",
    "### üèÜ Resumen de Logros\n",
    "¬°Felicidades! En este notebook has aprendido:\n",
    "\n",
    "1. **Boosting vs Bagging:** Secuencial vs paralelo, reducci√≥n de sesgo vs varianza.\n",
    "2. **Residuales:** Cada √°rbol aprende de los errores del modelo actual.\n",
    "3. **Learning Rate:** Peque√±o Œ∑ + muchos √°rboles = mejor generalizaci√≥n.\n",
    "4. **XGBoost:** Regularizaci√≥n + missing values + paralelizaci√≥n.\n",
    "5. **LightGBM:** Leaf-wise growth + histograms = velocidad extrema.\n",
    "\n",
    "### ‚úÖ Cu√°ndo Usar Gradient Boosting\n",
    "- Datos tabulares (no im√°genes/texto)\n",
    "- Cuando la precisi√≥n es m√°s importante que la interpretabilidad\n",
    "- Competencias de ML (Kaggle)\n",
    "\n",
    "### ‚ö†Ô∏è Cuidado con...\n",
    "- Overfitting (usa early stopping)\n",
    "- Tuning excesivo (puede memorizar el validation set)\n",
    "- Extrapolaci√≥n (no funciona bien fuera del rango de datos)\n",
    "\n",
    "---\n",
    "\n",
    "### üëâ Siguiente Paso\n",
    "Hemos cubierto modelos basados en √°rboles. Pero existen otros enfoques:\n",
    "\n",
    "**SVM (Support Vector Machines):** Encontrar el hiperplano que maximiza el margen entre clases.\n",
    "\n",
    "**KNN (K-Nearest Neighbors):** Clasificar bas√°ndose en los vecinos m√°s cercanos.\n",
    "\n",
    "*En el siguiente notebook veremos estos algoritmos \"geom√©tricos\" que no usan √°rboles.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
